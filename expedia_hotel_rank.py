# -*- coding: utf-8 -*-
"""Impedihierbabuena.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uVgT7Bz7qC2i_ixomWliZzhs7cDsFHFO
"""



!pip install pyltr

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import csv
import keras
import tensorflow
import pickle
import seaborn as sns
import time
import matplotlib
import pickle
import pyltr
import joblib

from pyltr.models import LambdaMART 

#from learning2rank.rank import RankNet
#from learning2rank.rank import ListNet

from datetime import datetime
from datetime import timedelta

from sklearn import *
from sklearn import datasets
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE

from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LassoCV

from sklearn.svm import SVR
from sklearn.svm import SVC
from sklearn.svm import LinearSVC

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV

from sklearn.decomposition import PCA
from sklearn.multiclass import OneVsRestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import make_scorer

!pip install -U -q kaggle
!mkdir -p ~/.kaggle
from google.colab import files
files.upload()

!cp kaggle.json ~/.kaggle/
!kaggle competitions download -c vu-dmt-2assignment
!ls

tren = pd.read_csv('training_set_VU_DM.csv.zip', compression='zip')

test = pd.read_csv('test_set_VU_DM.csv.zip', compression='zip')

def addUp(data):
    #Means the rating scores
    data['overall_rate'] = np.mean(data[['prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2']], axis = 1)
    
    #Multiplies competitors rates to get 'gain'
    price = data.columns.get_loc('price_usd') #15
    init = data.columns.get_loc('comp1_rate') #27
    end = data.columns.get_loc('comp8_rate_percent_diff') #50

    while init <= end:
      data.iloc[:,init] = data.iloc[:,init] * data.iloc[:,init + 2] * (data.iloc[:,price]/100)
      data.iloc[:,init] = data.iloc[:,init].fillna(0)
      data = data.drop(data.columns[init + 2], axis = 1)

      init = init + 2
      end = end - 1
    return data

def high_season(data, flag = 0):
    #Transform to date_time
    data['date_time'] = datetime.strptime(str(data['date_time'][0]), '%Y-%m-%d %X')
    
    #Add srch_booking days to actual time
    data['book_start'] = pd.to_datetime(data['date_time']) + pd.to_timedelta(data['srch_booking_window'].values, unit='days')
    
    
    data['month_season'] = data['book_start'].dt.month
    data['in_high_season'] = 0
    
    #Turns value to 1 if in high season
    data.loc[data['month_season'] == 4, 'in_high_season'] = 1
    data.loc[data['month_season'] == 5, 'in_high_season'] = 1
    data.loc[data['month_season'] == 6, 'in_high_season'] = 1

    data = data.drop('month_season', axis = 1)
    data = data.drop('book_start', axis = 1)
    
    return data

def same_country(data):
    data['same_country'] = 0
    data.loc[data['visitor_location_country_id'] == data['prop_country_id'], 'same_country'] = 1
    return data

def normal_price(data):
    data.loc[data['price_usd'] > 300, 'price_usd'] = data['price_usd']/data['srch_length_of_stay']
    return data

def clean(data, flag = 0): 
    to_delete = ['date_time',
                 'site_id',
                 'prop_review_score',
                 'prop_location_score1',
                 'prop_location_score2',
                 'prop_country_id','prop_id',
                 'random_bool']
    
    if(flag == 0):
        to_delete.extend(['booking_bool', 'click_bool', 'position'])
    
    data = data.drop(to_delete, axis = 1)
    data = data[data.loc[:, data.isnull().mean() < .75].columns]

    data.fillna(0, inplace = True)
    print("Features eliminated due to excess of NaN: ", data.loc[:, data.isnull().mean() > .75].columns)
    
    return data

def preprocess(data, flag):
    data = addUp(data)
    data = high_season(data, flag)
    data = same_country(data)
    data = clean(data)
    return data

y = tren['click_bool']
X = tren[test.columns]
X = X.drop('date_time', axis = 1)

X = X.fillna(0)

del tren
del test

X = preprocess(tren, 0)

def yterm(trenecito):
  trenecito['score'] = 0
  trenecito.loc[trenecito.click_bool == 1, 'score'] = 1
  trenecito.loc[trenecito.booking_bool == 1, 'score'] = 5
  return trenecito

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=None)

rfc_opt=RandomForestClassifier(random_state=42,
                               max_features='auto',
                               n_estimators=50,
                               max_depth=0.5,
                               min_samples_split=20,
                              )

model = rfc_opt.fit(X_train, y_train)

joblib.dump(model, "modelito_rf.sav")
pickle.dump(model, open("modelito_rf.pkl", "wb"), protocol = 2)

rfc_opt.score(X_val, y_val)

#no of features
nof_list=np.arange(1,49)            
high_score=0
#Variable to store the optimum features
nof=0           
score_list =[]
for n in range(len(nof_list)):
    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)
    model = LinearRegression()
    rfe = RFE(model,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train,y_train)
    X_test_rfe = rfe.transform(X_test)
    model.fit(X_train_rfe,y_train)
    score = model.score(X_test_rfe,y_test)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))

X_train = X_train.fillna(0)
X_val = X_val.fillna(0)

X_train = X_train.sort_values(["srch_id"])
X_val = X_val.sort_values(["srch_id"])

X_train = X_train.rename(columns={"srch_id":"qid"})
X_val = X_val.rename(columns={"srch_id":"qid"})

qid_train = X_train['qid'].values
qid_val = X_val['qid'].values

metric = pyltr.metrics.NDCG(k=10)

# Only needed if you want to perform validation (early stopping & trimming)
#monitor = pyltr.models.monitors.ValidationMonitor(X_val, y_val, qid_val, metric=metric, stop_after=250)

model = pyltr.models.LambdaMART(
    metric = metric,
    n_estimators = 10,
    learning_rate = 0.01,
    max_features = 0.5,
    max_leaf_nodes = 15,
    min_samples_leaf = 100,
    verbose = 1,
)

model.fit(X_train, y_train, qid_train)

pickle.dump(model, open("modelito.pkl", "wb"))

final_test = test.drop('date_time', axis = 1).fillna(0)

del test

prediction = model.predict(final_test)

final = X_train[['qid', 'prop_id']]

final['pred'] = pd.DataFrame(prediction)

pred = final.sort_values(["srch_id", "pred"], ascending = False)

X_train

b = pd.DataFrame(a['qid'])

b['prop_id'] = test['prop_id']

b['pred'] = pd.DataFrame(prediction)

c = b.reset_index()

c.sort_values(["qid", "pred"], ascending = [True, False])

d = c.drop(['pred', 'index'], axis = 1)

d.to_csv("pred_prepro_Lambda3.csv", index=False)

